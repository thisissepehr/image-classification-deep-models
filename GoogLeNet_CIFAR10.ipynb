{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **GoogLeNet very deep convolutional Model**"
      ],
      "metadata": {
        "id": "VbuHfTpGEOkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.Installing and importing requirements**"
      ],
      "metadata": {
        "id": "OCnfj6BXEZ5L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLGbtDOXEI0i",
        "outputId": "4e1eeda1-e331-4601-8f57-886ad660355d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ali\n"
          ]
        }
      ],
      "source": [
        "! pip install \"ray[tune]\" torch torchvision pytorch-lightning\n",
        "'''\n",
        "Ray Tune is a Python library that accelerates hyperparameter tuning by allowing\n",
        "you to leverage cutting edge optimization algorithms at scale.\n",
        "\n",
        "PyTorch Lighting is a lightweight PyTorch wrapper for high-performance AI research\n",
        "that aims to abstract Deep Learning boilerplate while providing you full control\n",
        "and flexibility over your code \n",
        "\n",
        "'''\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\n",
        "'''\n",
        "preparing to use the google cloud tpu capabilities\n",
        "'''\n",
        "\n",
        "!nvidia-smi # checking nvidia system managment interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaFjK_slEI0l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import gc\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from tqdm.autonotebook import tqdm\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print(pl.__version__)"
      ],
      "metadata": {
        "id": "ctt-DHT9FfUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.HyperParameters**\n"
      ],
      "metadata": {
        "id": "71BAXh74F3ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "RANDOM_SEED = 42\n",
        "LEARNING_RATE = 1e-2\n",
        "BATCH_SIZE = 2\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Architecture\n",
        "NUM_FEATURES = 28*28\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Other\n",
        "DEVICE = \"cuda:0\"\n",
        "GRAYSCALE = False"
      ],
      "metadata": {
        "id": "VQaQOMo3F6Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "G19EuALGGJu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        ])\n",
        "\n",
        "train_set = datasets.CIFAR10(root='data', \n",
        "                               train=True, \n",
        "                               transform=transform,\n",
        "                               download=True)\n",
        "\n",
        "test_set = datasets.CIFAR10(root='data', \n",
        "                              train=False, \n",
        "                              transform=transform)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_set, \n",
        "                          batch_size=BATCH_SIZE, \n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_set, \n",
        "                         batch_size=BATCH_SIZE, \n",
        "                         shuffle=False)\n",
        "images, labels = next(iter(train_loader):  \n",
        "print('Image batch dimensions:', images.shape)\n",
        "print('Image label dimensions:', labels.shape)\n"
      ],
      "metadata": {
        "id": "rbMUDA6WGQ0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images , _ = next(iter(train_loader))\n",
        "print('images.shape:', images.shape)\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.axis('off')\n",
        "plt.imshow(torchvision.utils.make_grid(images, nrow=16).permute((1, 2, 0))"
      ],
      "metadata": {
        "id": "fsggEJjvLJ0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.Model**\n"
      ],
      "metadata": {
        "id": "6DIQZVWWLhx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Building small blocks for the Inception Model**\n"
      ],
      "metadata": {
        "id": "pEstQ5DLLpXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k, s, p):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.convolution = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_ch,\n",
        "                      out_channels=out_ch,\n",
        "                      kernel_size=(k, k),\n",
        "                      stride=(s, s),\n",
        "                      padding=(p, p)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convolution(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ReducedConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch1,out_ch2, k, p):\n",
        "        super(ReducedConvBlock, self).__init__()\n",
        "        self.reducedConv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_ch,\n",
        "                      out_channels= out_ch1,\n",
        "                      kernel_size=(1,1),\n",
        "                      stride=(1,1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels= out_ch1,\n",
        "                      out_channels= out_ch2,\n",
        "                      kernel_size=(k, k),\n",
        "                      stride=(1, 1),\n",
        "                      padding=(p, p)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reducedConv(x)\n",
        "        return x\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, curr_in_fts, f_1x1, f_3x3_r, f_3x3, f_5x5_r, f_5x5, f_pool_proj):\n",
        "        super(InceptionModule, self).__init__()\n",
        "        self.conv1 = ConvBlock(curr_in_fts, f_1x1, 1, 1, 0)\n",
        "        self.conv2 = ReducedConvBlock(curr_in_fts, f_3x3_r, f_3x3, 3, 1)\n",
        "        self.conv3 = ReducedConvBlock(curr_in_fts, f_5x5_r, f_5x5, 5, 2)\n",
        "\n",
        "        self.pool_proj = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=(1, 1), stride=(1, 1)),\n",
        "            nn.Conv2d(in_channels=curr_in_fts,\n",
        "                      out_channels=f_pool_proj,\n",
        "                      kernel_size=(1, 1),\n",
        "                      stride=(1, 1)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        out1 = self.conv1(input_img)\n",
        "        out2 = self.conv2(input_img)\n",
        "        out3 = self.conv3(input_img)\n",
        "        out4 = self.pool_proj(input_img)\n",
        "\n",
        "\n",
        "        x = torch.cat([out1, out2, out3, out4], dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class AuxClassifier(nn.Module):\n",
        "    def __init__(self, in_fts, num_classes):\n",
        "        super(AuxClassifier, self).__init__()\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=(5, 5), stride=(3, 3))\n",
        "        self.conv = nn.Conv2d(in_channels=in_fts,\n",
        "                              out_channels=128,\n",
        "                              kernel_size=(1, 1),\n",
        "                              stride=(1, 1))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Linear(4 * 4 * 128, 1024)\n",
        "        self.dropout = nn.Dropout(p=0.7)\n",
        "        self.classifier = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        N = input_img.shape[0]\n",
        "        x = self.avgpool(input_img)\n",
        "        x = self.conv(x)\n",
        "        x = self.relu(x)\n",
        "        x = x.reshape(N, -1)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4DAfrLE2LnEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GoogLeNet**"
      ],
      "metadata": {
        "id": "NbF5c83HMKm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GoogleNet(nn.Module):\n",
        "    def __init__(self, in_fts=3, num_class=10):\n",
        "        super(GoogleNet, self).__init__()\n",
        "        self.conv1 = ConvBlock(in_fts, 64, 7, 2, 3)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.conv2 = nn.Sequential(\n",
        "            ConvBlock(64, 64, 1, 1, 0),\n",
        "            ConvBlock(64, 192, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "        self.inception_3a = InceptionModule(192, 64, 96, 128, 16, 32, 32)\n",
        "        self.inception_3b = InceptionModule(256, 128, 128, 192, 32, 96, 64)\n",
        "        self.inception_4a = InceptionModule(480, 192, 96, 208, 16, 48, 64)\n",
        "        self.inception_4b = InceptionModule(512, 160, 112, 224, 24, 64, 64)\n",
        "        self.inception_4c = InceptionModule(512, 128, 128, 256, 24, 64, 64)\n",
        "        self.inception_4d = InceptionModule(512, 112, 144, 288, 32, 64, 64)\n",
        "        self.inception_4e = InceptionModule(528, 256, 160, 320, 32, 128, 128)\n",
        "        self.inception_5a = InceptionModule(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.inception_5b = InceptionModule(832, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "        self.aux_classifier1 = AuxClassifier(512, num_class)\n",
        "        self.aux_classifier2 = AuxClassifier(528, num_class)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.Linear(1024 * 7 * 7, num_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        N = input_img.shape[0]\n",
        "        x = self.conv1(input_img)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.inception_3a(x)\n",
        "        x = self.inception_3b(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.inception_4a(x)\n",
        "        out1 = self.aux_classifier1(x)\n",
        "        x = self.inception_4b(x)\n",
        "        x = self.inception_4c(x)\n",
        "        x = self.inception_4d(x)\n",
        "        out2 = self.aux_classifier2(x)\n",
        "        x = self.inception_4e(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.inception_5a(x)\n",
        "        x = self.inception_5b(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(N, -1)\n",
        "        x = self.classifier(x)\n",
        "        if self.training == True:\n",
        "            return [x, out1, out2]\n",
        "        else:\n",
        "            return x"
      ],
      "metadata": {
        "id": "6lTJG55cMNeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.Train and logging**"
      ],
      "metadata": {
        "id": "2fuxSIROMb0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GoogLeNet_PL(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # define model and loss\n",
        "    self.model = GoogleNet()\n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "  \n",
        "  def custom_histogram_adder(self):\n",
        "    for name,params in self.named_parameters():\n",
        "      self.logger.experiment.add_histogram(name,params,self.current_epoch)\n",
        "  \n",
        "  def training_step(self, batch, batch_no):\n",
        "    x, y = batch\n",
        "\n",
        "    # GoogLeNet training loss from auxiliary classifiers\n",
        "    outputs, aux1_outputs, aux2_outputs = self(x)\n",
        "    discount = 0.3\n",
        "    loss = (self.loss(outputs, y) + discount * (self.loss(aux1_outputs, y) + self.loss(aux2_outputs, y)))\n",
        "\n",
        "\n",
        "    pred = self.forward(x)[0]\n",
        "    correct=pred.argmax(dim=1).eq(y).sum().item()\n",
        "    total=len(y)\n",
        "    batch_dictionary={\n",
        "            #REQUIRED: It ie required for us to return \"loss\"\n",
        "            \"loss\": loss,\n",
        "            # info to be used at epoch end \n",
        "            \"correct\": correct,\n",
        "            \"total\": total\n",
        "        }\n",
        "    #self.log('train_loss', loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "    return batch_dictionary\n",
        "  \n",
        "  def training_epoch_end(self,outputs):\n",
        "        #  the function is called after every epoch is completed\n",
        "\n",
        "        # calculating average loss  \n",
        "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "\n",
        "        # calculating correct and total predictions\n",
        "        correct=sum([x[\"correct\"] for  x in outputs])\n",
        "        total=sum([x[\"total\"] for  x in outputs])\n",
        "        self.custom_histogram_adder()\n",
        "        # creating log dictionary\n",
        "        self.logger.experiment.add_scalar(\"Loss/Train\",\n",
        "                                            avg_loss,\n",
        "                                            self.current_epoch)\n",
        "        \n",
        "        self.logger.experiment.add_scalar(\"Accuracy/Train\",\n",
        "                                            correct/total,\n",
        "                                            self.current_epoch)\n",
        "\n",
        "        return None\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    # choose your optimizer\n",
        "    return torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "Jx_bwvqCMZIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = TensorBoardLogger('tensorlogs', name = 'googlenet_CIFAR10Model')\n",
        "model = GoogLeNet_PL() \n",
        "trainer = pl.Trainer(\n",
        "    tpu_cores=8, # use one GPU\n",
        "    max_epochs=NUM_EPOCHS, # set number of epochs\n",
        "    progress_bar_refresh_rate=20,\n",
        "    logger = logger\n",
        ")"
      ],
      "metadata": {
        "id": "X_MbjtgqM3fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model, train_loader)\n"
      ],
      "metadata": {
        "id": "jTTfIe1iNAck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tensorlogs/"
      ],
      "metadata": {
        "id": "Tkg0WTMrNI5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.Evaluation**"
      ],
      "metadata": {
        "id": "RbxIKMRANMRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(x, model: pl.LightningModule):\n",
        "  model.freeze() # prepares model for predicting\n",
        "  probabilities = torch.softmax(model(x), dim=1)\n",
        "  predicted_class = torch.argmax(probabilities, dim=1)\n",
        "  return predicted_class, probabilities"
      ],
      "metadata": {
        "id": "uSEx0WTONOte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_y, pred_y = [], []\n",
        "for batch in tqdm(iter(test_dl), total=len(test_dl)):\n",
        "  x, y = batch\n",
        "  true_y.extend(y)\n",
        "  #print(type(x))\n",
        "  preds, probs = get_prediction(x, model)\n",
        "  pred_y.extend(preds.cpu())"
      ],
      "metadata": {
        "id": "3rLba5tQNUb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test results\n",
        "print(classification_report(true_y, pred_y, digits=3))"
      ],
      "metadata": {
        "id": "nd7wWOcsNXxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features, targets = next(iter(test_loader))\n",
        "\n",
        "features = features\n",
        "targets = targets\n",
        "\n",
        "np_features = features[0].numpy()\n",
        "nhwc_img = np.transpose(np_features, axes=(1, 2, 0))\n",
        "plt.imshow(nhwc_img)"
      ],
      "metadata": {
        "id": "PIlzOg01RPjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "predicted, probability = get_prediction(features[0, None], model)\n",
        "print('Ground Truth Label',classes[targets[0]])\n",
        "print('Predicted:', classes[predicted])\n",
        "print('Probability:', probability[0][predicted]*100)"
      ],
      "metadata": {
        "id": "CvC-56J8Ro-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/file.zip /content/tensorlogs/\n"
      ],
      "metadata": {
        "id": "TEojYWjYRr2T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "GoogLeNet_CIFAR10.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}